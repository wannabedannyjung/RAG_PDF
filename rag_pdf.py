##############################################################
# RAG over PDF with FAISS & Streamlit
# 
# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì»¤ë§¨ë“œ
# ì£¼ì˜ì  : ë¯¸ë‹ˆì½˜ë‹¤ ê°€ìƒí™˜ê²½ì„ ìƒˆë¡œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•  ê²ƒ, LangFlow í™˜ê²½ì— ì„¤ì¹˜ì‹œ ê¸°ì¡´ ê°œë°œ í™˜ê²½ê³¼ ì¶©ëŒ ë°œìƒ
# ì¶”ì²œ Python version : 3.10
# pip install uv
# uv pip install -U streamlit langchain langchain-community langchain-openai sentence-transformers faiss-cpu pypdf
# pip install -U langchain-huggingface
#
# ì‹¤í–‰ ë°©ë²• : # streamlit run rag_pdf.py
#
# ë¡œì»¬ LLM ëª¨ë¸ ì‚¬ìš©ì‹œ ì˜¬ë¼ë§ˆ ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•´ë†“ì„ ê²ƒ
# https://ollama.com/download
##############################################################

import os
import io
import tempfile
from typing import List

# OpenAI ì‚¬ìš©ì‹œ API í‚¤ ì§ì ‘ ì„¤ì •, "ì˜¬ë¼ë§ˆ"ë¡œ ë¡œì»¬ëª¨ë¸ ì‚¬ìš©ì‹œëŠ” ì…‹íŒ…ì•ˆí•´ë„ ë¨.
os.environ['OPENAI_API_KEY'] = ''

# ìŠ¤íŠ¸ë¦¼ë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st

# íŒŒì´ì„  ë¡œë”
from langchain_community.document_loaders import PyPDFLoader

# ë²¡í„°ë””ë¹„ ìƒì„±ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# LLM - OpenAI ë˜ëŠ” Ollama(ë¡œì»¬)
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama

# UI ì„¤ì •
st.set_page_config(page_title="PDF RAG with FAISS", page_icon="ğŸ“š", layout="wide")
# ì´ëª¨ì§€ ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ë©´? https://emojipedia.org/ ì—ì„œ ì´ëª¨ì§€ ë³µì‚¬í•´ì„œ ë¶™ì—¬ì„œ ì‚¬ìš©, ì´ëª¨ì§€ëŠ” ì»¬ëŸ¬ í°íŠ¸ Windows â†’ Segoe UI Emoji
st.title("âœ¨ ì—¬ëŸ¬ PDF ë¥¼ ë¡œë”©í•˜ê³ , ê°ê° ì§ˆë¬¸í•´ ë´…ë‹ˆë‹¤.")

# UI ì„¤ì • - ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")

    provider = st.selectbox(
        "LLM ì„ íƒ",
        [
            "Ollama (ë¡œì»¬)",
            "OpenAI (í´ë¼ìš°ë“œ)",
        ],
        index=1,
    )

    model_name = st.text_input(
        "ëª¨ë¸ ì´ë¦„",
        # ì˜¬ë¼ë§ˆ ì„¤ì¹˜ ë° ollama run exaone3.5:2.4b ë“±ìœ¼ë¡œ ëª¨ë¸ ë¡œë”©ì´ ì„ í–‰ë˜ì–´ì•¼ í•¨
        # exaone3.5:2.4b  ì—‘ì‚¬ì› ëª¨ë¸ì˜ ê²½ìš° LG AI Research íŒ€ì—ì„œ ê°œë°œí•œ ê²ƒìœ¼ë¡œ, ì‘ì€ ìš©ëŸ‰ìœ¼ë¡œ gtx1050 ì—ì„œë„ ëŒë¦´ìˆ˜ ìˆëŠ” ê²½ëŸ‰ ëª¨ë¸, GPU 4GB ì •ë„ ì‚¬ìš©
        value="exaone3.5:2.4b" if provider.startswith("Ollama") else "gpt-4o",     # "gpt-4o-mini"
        help="OllamaëŠ” ë¡œì»¬ì— í•´ë‹¹ ëª¨ë¸ì´ pull ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. OpenAIëŠ” API Key í•„ìš”",
    )

    embed_model = st.text_input(
        "ì„ë² ë”© ëª¨ë¸ (Sentence-Transformers)",
        value="sentence-transformers/all-MiniLM-L6-v2",
        #value="BAAI/bge-large-en-v1.5",
        help="ì„ë² ë”© ëª¨ë¸ì— ë”°ë¼ì„œ gpu ì‚¬ìš©, ì°¨ì›ì¦ê°€ë¡œ ë¹„ìš©ì¦ê°€, í’ˆì§ˆì°¨ì´ê°€ ë°œìƒ",
    )

    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 300, 2000, 850, 50)
    chunk_overlap = st.slider("ì²­í¬ ì˜¤ë²„ë©", 0, 400, 90, 10)
    top_k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (k)", 1, 20, 16, 1)   # ê²€ìƒ‰ ë¬¸ì„œ í›„ë³´ ìˆ˜

    persist = st.checkbox("FAISS ì¸ë±ìŠ¤ ë””ìŠ¤í¬ ì €ì¥(.faiss_index)", value=True)

# UI ì„¤ì • - 
st.markdown(
    """
> ê²€ìƒ‰ ì¦ê°• ìƒì„± - RAG (Retrieval-Augmented Generation) ì›ë¦¬ 
>
> PDF ë¬¸ì„œ Thunking -> FAISS Vector DB -> (ì§ˆì˜ ì‹œì ) DB Retriever ë¡œ TOP_K ê°¯ìˆ˜ë§Œí¼ ìœ ì‚¬í•œ Chunk Return -> ë¦¬í„´ëœ Chunk ë¥¼ LLM ì´ ì¶”ë¡  
>
> Chunk ì‚¬ì´ì¦ˆë¥¼ 1000, Overlap ì„ 100, TOP_K ë¥¼ 10 ê°œë¡œ ë³€ê²½ í›„ ë‹¤ì‹œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”.
>
> ì„±ëŠ¥ ì¢‹ì€ ì„ë² ë”© ëª¨ë¸ ë° ì¶”ë¡ ëª¨ë¸ ì‚¬ìš©ì‹œ ê²€ìƒ‰ í’ˆì§ˆì´ ì¢‹ì•„ì§€ë‚˜, GPU ë° í•˜ë“œì›¨ì–´ê°€ í•„ìš”í•œ ì‹œì ì´ ìƒê¹ë‹ˆë‹¤.

**ì‚¬ìš©ë²•**
1) ì™¼ìª½ì—ì„œ LLM/ì„ë² ë”©/ì²­í¬ ì„¤ì •ì„ ê³ ë¦…ë‹ˆë‹¤.  
2) ì•„ë˜ ì˜ì—­ì— PDF íŒŒì¼(ë³µìˆ˜ ê°€ëŠ¥)ì„ ì—…ë¡œë“œí•˜ê³  **ì¸ë±ìŠ¤ ìƒì„±**ì„ ëˆ„ë¦…ë‹ˆë‹¤.
3) ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ RAGë¡œ ë‹µë³€ê³¼ ì¶œì²˜(í˜ì´ì§€)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
)

# st.session_state : ë¸Œë¼ìš°ì ¸ ì„¸ì…˜ë³„ ê³ ìœ í•œ ìƒíƒœ ê°’ì‚¬ìš©ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
if "faiss" not in st.session_state:
    st.session_state.faiss = None   # Vector DB
if "docs_meta" not in st.session_state:
    st.session_state.docs_meta = [] # Vector DB Meta

# íŒŒì¼ ì—…ë¡œë” ì½˜íŠ¸ë¡¤ - BytesIO ìŠ¤íŠ¸ë¦¼ íƒ€ì…
uploaded_files = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True)

# ì¸ë±ìŠ¤ ìƒì„± / ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì½˜íŠ¸ë¡¤
col_build, col_reset = st.columns([1, 1])

# ë¦¬ì†ŒìŠ¤ ìºì‹± ë°ì½”ë ˆì´í„°, Streamlit ì„œë²„ê°€ ì‚´ì•„ìˆëŠ” ë™ì•ˆ (ì•± ë¦¬ë¡œë“œ ì‹œ ìºì‹œ ìœ ì§€) ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°ë¡œë“œì‹œ í•œë²ˆë§Œ ë¡œë“œí•˜ê³ , ì¬ì‚¬ìš©
@st.cache_resource(show_spinner=False)
def get_embeddings(model_name: str):
    # HuggingFaceEmbeddingsëŠ” sentence-transformers ê¸°ë°˜
    return HuggingFaceEmbeddings(model_name=model_name)

def load_pdfs_to_docs(files: List[io.BytesIO]):
    docs_all, meta_all = [], []

    for f in files:
        strFileName = getattr(f, "name", "uploaded.pdf")
        print("íŒŒì¼ ì²˜ë¦¬ì¤‘...", strFileName)

        # A) ì—…ë¡œë” í¬ì¸í„° ì´ˆê¸°í™”
        try:
            f.seek(0)
        except Exception:
            pass

        # ì›ë³¸ ì´ë¦„ìœ¼ë¡œ ì„ì‹œ ì €ì¥(ì„ íƒ) â€” temp ê²½ë¡œ ë•Œë¬¸ì— í—·ê°ˆë¦¬ë©´ ê¶Œì¥
        tmp_dir = tempfile.gettempdir()
        tmp_path = os.path.join(tmp_dir, f"uploaded_{os.path.basename(strFileName)}")
        with open(tmp_path, "wb") as out:
            out.write(f.read())

        loader = PyPDFLoader(tmp_path)
        docs = loader.load()

        cleaned_docs = []
        for d in docs:
            text = (d.page_content or "").strip()
            if not text:                 # B) ë¹ˆ í…ìŠ¤íŠ¸ í˜ì´ì§€ ì œê±°
                continue
            d.metadata["source"] = strFileName  # temp ê²½ë¡œ â†’ ì›ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ ê³ ì •
            pg = d.metadata.get("page", d.metadata.get("page_number", None))
            meta_all.append({
                "source": strFileName,
                "page": int(pg) + 1 if isinstance(pg, int) else pg,
                "content": text[:200].replace("\n", " ")
            })
            cleaned_docs.append(d)

        docs_all.extend(cleaned_docs)

    return docs_all, meta_all

# FAISS Vector Store ë§Œë“¤ê¸°
def build_faiss_index(docs, embedding_model_name: str, chunk_size: int, chunk_overlap: int):
    # RecursiveCharacterTextSplitter ì„ í†µí•´ ì˜ë¯¸ê°€ ëŠê¸°ì§€ ì•Šê²Œ Chunk ë¥¼ ì˜ë¼ì¤€ë‹¤. separators ì˜ ìˆœì„œëŒ€ë¡œ Try
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""]
    )
    # RecursiveCharacterTextSplitter ì„ ì‚¬ìš©í•´ì„œ ì²­í¬ ë§Œë“¤ê¸°
    chunks = splitter.split_documents(docs)
    
    # ì„ë² ë”©ìš© ëª¨ë¸
    embeddings = get_embeddings(embedding_model_name)

    # ì„ë² ë”©ìš© ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ FAISS ì—ì„œ Vector Store ë¥¼ ìƒì„±
    vs = FAISS.from_documents(chunks, embedding=embeddings)
    
    return vs

with col_build:
    if st.button("ğŸ”¨ ë²¡í„° DB ìƒì„±", use_container_width=True, type="primary"):
        if not uploaded_files:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            with st.spinner("ì„ë² ë”© & ì¸ë±ìŠ¤ ìƒì„± ì¤‘..."):
                # íŒŒì¼ ë¡œë“œ - docs, meta ì •ë³´ 
                docs, meta = load_pdfs_to_docs(uploaded_files)
                
                # ì¸ë±ìŠ¤ ë¹Œë“œ
                vs = build_faiss_index(docs, embed_model, chunk_size, chunk_overlap)
                
                # ë²¡í„°ìŠ¤í† ì–´
                st.session_state.faiss = vs
                # ë©”íƒ€ì •ë³´
                st.session_state.docs_meta = meta
                
                # ë””ìŠ¤í¬ ì €ì¥
                if persist:
                    FAISS.save_local(vs, folder_path=".faiss_index")

            st.success("FAISS DB ìƒì„± ì™„ë£Œ!")

with col_reset:
    if st.button("â™»ï¸ ë²¡í„° DB ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.faiss = None
        st.session_state.docs_meta = []
        st.success("ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê¸°ì¡´ ì €ì¥ ì¸ë±ìŠ¤ ë¡œë“œ ë²„íŠ¼
col_load, col_dummy = st.columns([1, 3])

with col_load:
    if st.button("ğŸ’¾ ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ(.faiss_index)"):
        try:
            embeddings = get_embeddings(embed_model)

            vs = FAISS.load_local(
                folder_path=".faiss_index",
                embeddings=embeddings,
                allow_dangerous_deserialization=True,
            )

            st.session_state.faiss = vs
            st.info("ì €ì¥ëœ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ë¡œë“œ ì‹¤íŒ¨: {e}")

st.divider()

st.markdown(
    """
    ### ì˜ˆì‹œ ì§ˆë¬¸
    > ì†Œë‚˜ê¸°ì˜ ì£¼ì¸ê³µì€?, ì†Œë…€ëŠ” ëˆ„êµ¬ì¸ê°€?
    >
    > ê²½ì˜ì§€ì› ì‹ ì…ì‚¬ì› JOB DESCRIPTION ì„ ì•Œë ¤ì£¼ì„¸ìš”
    >
    > ì—ì–´ì»¨ì— ë¬¼ì´ ìƒ™ë‹ˆë‹¤, ì‚¼ì„± ì—ì–´ì»¨ ì„œë¹„ìŠ¤ ì „í™”ë²ˆí˜¸ ì•Œë ¤ì£¼ì„¸ìš”
    """)

# ì§ˆì˜/ì‘ë‹µ
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì†Œë‚˜ê¸°ì˜ ì£¼ì¸ê³µì€?, ê²½ì˜ì§€ì› ì‹ ì…ì‚¬ì› JOB DESCRIPTION ì„ ì•Œë ¤ì£¼ì„¸ìš”, ì—ì–´ì»¨ì— ë¬¼ì´ ìƒ™ë‹ˆë‹¤")

# LLM ì¤€ë¹„
def get_llm():
    if provider.startswith("Ollama"):
        if ChatOllama is None:
            st.error("ChatOllamaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'langchain-community' íŒ¨í‚¤ì§€ì™€ Ollamaê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()
        return ChatOllama(model=model_name, temperature=0.1)
    else:
        if ChatOpenAI is None:
            st.error("OpenAI íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: 'langchain-openai'")
            st.stop()
        # í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEYê°€ ìˆì–´ì•¼ í•¨
        if not os.getenv("OPENAI_API_KEY"):
            st.warning("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return ChatOpenAI(model=model_name, temperature=0.1)

# RAG ìš© í”„ë¡¬í”„íŠ¸ ì„¤ì •
#
# RAG ì‹œ ì¤‘ìš” í”„ë¡¬í”„íŠ¸ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ë° ì¶œì²˜ ì¶œë ¥
# "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
# "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”."
def make_rag_prompt(question: str, context_chunks: List[str]):
    joined = "\n\n".join([f"[ì»¨í…ìŠ¤íŠ¸ {i+1}]\n" + c for i, c in enumerate(context_chunks)])
    
    sys = (
        "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
        "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”."
    )
    user = f"ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{joined}\n\në‹µë³€:"

    return sys, user

if query:
    if st.session_state.faiss is None:
        st.warning("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±(ë˜ëŠ” ë¡œë“œ)í•˜ì„¸ìš”.")
    else:
        # ê²€ìƒ‰
        retriever = st.session_state.faiss.as_retriever(search_kwargs={"k": top_k})
        
        docs = retriever.invoke(query)
        
        if not docs:  # C) 0ê±´ ê°€ë“œ
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ êµ¬ì²´í™”í•˜ê±°ë‚˜ ì²­í¬/ì„ë² ë”© ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”.")
            st.stop()

        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê¸¸ì´ ì œí•œ)
        contexts: List[str] = []
        sources = []
        for d in docs:
            text = d.page_content
            meta = d.metadata or {}
            page = meta.get("page", meta.get("page_number", None))
            if isinstance(page, int):
                page = page + 1
            src = meta.get("source", "PDF")
            # ì§§ì€ ìŠ¤ë‹ˆí«
            snippet = (text[:300] + "...") if len(text) > 300 else text
            contexts.append(snippet)
            sources.append({"source": os.path.basename(src), "page": page, "snippet": snippet})

        llm = get_llm()
        system_prompt, user_prompt = make_rag_prompt(query, contexts)

        with st.spinner("LLM ì‘ë‹µ ìƒì„± ì¤‘..."):
            # Chat API ìŠ¤íƒ€ì¼ í˜¸ì¶œ
            response = llm.invoke([
                ("system", system_prompt),
                ("user", user_prompt),
            ])
            answer = response.content if hasattr(response, "content") else str(response)

        # ì¶œë ¥
        st.subheader("ğŸ” ë‹µë³€")
        st.write(answer)

        with st.expander("ğŸ“Œ ì¶œì²˜ (Top-k ë¬¸ì„œ) ë³´ê¸°", expanded=False):
            for i, s in enumerate(sources, start=1):
                st.markdown(f"**{i}. {s['source']}** â€” p.{s['page']}\n\n> {s['snippet']}")

